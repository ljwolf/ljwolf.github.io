---
title: "Getting on another level"
output:
  tufte::tufte_html: default
  tufte::tufte_handout: default
---

Today, we'll use the `earnings` data on the relationship between race, height, sex, age, and earnings. The data is expressed as:

1. `earnings`, the yearly earnings (in $1000) for workers in a study
2. `height`, height in meters of the participant 
3. `sex`, one of `male`,`female`
4. `race`, racial category from the US Census, one of `white`, `black`, `asian`, `native`, `other`
5. `education`, the highest level of education attempted, one of `none`, `primary`, `secondary`, `university`, `postgraduate`, `phd`
6. `is_hispanic`, binary variable indicating whether the respondent identifies as a hispanic ethnicity
7. `age`, the age of the respondent. Must be over 18 to have participated in the study. 
8. `age_band`, a categorical variable indicating whether the respondent is 18 to 34, 35 to 50, or 50+. 
9. `was_promoted`, a categorical variable indicating whether the respondent was promoted recently. 

# 1. Doing Data Science Mentally

Fit a regression predicting earnings by height for two sexes, as well as the overall effect of height on earnings.

## 1.1 
Does an extra meter of height have a statistically significant impact on earnings for men? How about for women? Are the impacts *significantly different* from one another?

*First, let's do some cleaning & loading*
```{r, message=F, warning=F}
library(tidyverse)
library(broom)
earnings = read_csv("../data/earnings.csv")

earnings['height_centered'] = (earnings$height - mean(earnings$height, na.rm=T))
earnings['sex'] = as.factor(earnings$sex)
earnings['is_hispanic'] = as.factor(earnings$is_hispanic)
earnings['race'] = as.factor(earnings$race)
earnings['education'] = as.factor(earnings$education)
earnings['age_band'] = as.factor(earnings$age_band)

male_model = lm(earnings ~ height_centered, data=subset(earnings, sex=="male"))
female_model = lm(earnings ~ height_centered, data=subset(earnings, sex=="female"))
all_model = lm(earnings ~ height_centered, data=earnings)
```

*Then, making a big table of the results*:

```{r}
results = rbind(broom::tidy(male_model, conf.int=T) %>% 
                  mutate(model='men only'),
                broom::tidy(female_model, conf.int=T) %>% 
                  mutate(model='women only'), 
                broom::tidy(all_model, conf.int=T) %>% 
                  mutate(model='overall'))
```

*From this, we can see that the intercepts for each of the models are very different, but the slope estimates for men and women can't be told apart at the 95% confidence level. Also, the slope estimates for the overall model is way off of the estimate obtained from the groups separately...Something (like Simpson's Paradox) is going on!*

```{r echo=F}
knitr::kable(results %>% arrange(term), digits=3, align = 'l')
```

*If you wanted to show this graphically, one package that can do this is the dotwhisker package. However, this package can get a bit confusing to use with `lmer` models, so I recommend always using both textual and graphical displays!*
```{r}
dotwhisker::dwplot(results, show_intercept=TRUE)
```

## 1.2

Why would the impact of height on earnings be  *so much larger* for the data overall than for men or women as a subset?[^hint-scatter]

```{r, warning=F, message=F}
ggplot(earnings, aes(x=height, y=earnings, color=sex)) + 
  geom_point() + geom_smooth(method=lm) + # do the male/female split
  geom_smooth(aes(color='all'), method=lm) # re-write the aes to remove groups
```

*In order to fit the same line through both groups, the "overall" regression has to be much steeper. The association between height and earnings __within the groups__ is about the same, but when this heterogeneity is ignored, the association picks up a bit of the fact that men are, on average, taller than women & earning more than women in the dataset.*

## 1.3 
Fit a fixed effect model so that men and women have different baseline earnings, but so that height has the same effect on men and women. Does this give you a more reasonable estimate of the effect of height on earnings? Also, according to your regression: what is the difference in earnings for a 1.75 meter man vs. a 1.75 meter woman? 
```{r}
femodel = lm(earnings ~ 0 + height_centered + sex, data=earnings)
knitr::kable(tidy(femodel))
```
*__Yes!__ This model gives us a much more reasonable estimate because it accounts for the differences in the baseline height.*

```{r}
example_scenario = data.frame(height = c(1.75, 1.75), sex=c('female', 'male'))
example_scenario['height_centered'] = example_scenario$height - mean(earnings$height, na.rm=T)
example_scenario['predicted_earnings'] = predict(femodel, example_scenario)
knitr::kable(example_scenario)
```

*Note that the difference is __exactly__ the difference between the two baseline effects: `r round(diff(example_scenario$predicted_earnings), 3)`, since the slopes are the same for the two lines. A contrast model would show this directly in the `sexmale` effect:*
```{r}
lm(earnings ~ 1 + height_centered + sex, data=earnings) %>% tidy() %>% knitr::kable()
```

## 1.4
Fit a multilevel model using `lmer` that allows both the baseline earnings and the impact of height on earnings to vary. 

```{r message=F}
library(lme4)
library(broom.mixed)
mermod = lmer(earnings ~ height_centered + (1 + height_centered || sex), data=earnings)
```
*The fixed effects can be seen below:*
```{r}
knitr::kable(tidy(mermod))
```

*As in our scatterplots above, the mixed model sees a big difference in the baseline pay between men and women, but sees a very small (if negligible) difference in the effect of height on earnings between men and women: the effect is pretty much the same.*
```{r}
knitr::kable(tidy(mermod, effects="ran_coefs"))
```

*We can visualize this using the `ranef()` and `dotplot` as we did before in the workbook:*
```{r}
lattice::dotplot(ranef(mermod))
```

# 2

## 2.1
Fit a *generalized* mixed-effect model that predicts whether a respondent has income[^hint-hasincome], depending on their sex. Allow both the intercept and the effect of sex to vary by education level. 
*The actual replacement operation should be very straightforward. The first step is to create a binary variable which is `TRUE` wherever a person has non-zero earnings. The second step is to __remove__ or __replace__ the `NA` values with `0`, since if the income is missing, the income is not recorded. Functionally, this is two mutation operations, and they can be done in either order: `replace_na(earnings, 0)` and then `earnings > 0` or `earnings > 0` and then `replace_na(has_income, 0)`. This is done below, and I save the modification back to `earnings`:*
```{r}
earnings = earnings %>% 
  mutate(has_income = earnings > 0) %>%
  mutate(has_income = replace_na(has_income, 0))
```
*Then, the model is fit by combining the method you know from `glm` with the methods you know from `lmer`. This results in the following `glmer` command:*
```{r}
missing_model = glmer(has_income ~ 1 + sex + (1 + sex | education), 
                      data=earnings, family=binomial())
```
*Note that I'm now able to model the correlation between effects because I have more than two groups! It's OK if you didn't though, as that's not of substantive interest in this problem.*
## 2.1
What are the fixed and random component estimates for the model? 
```{r}
knitr::kable(tidy(missing_model))
```

```{r}
knitr::kable(tidy(missing_model, effects='ran_coefs'))
```

## 2.3 
What is the *predicted probability* a respondent in each group having their earnings recorded?[^hint-expandgrid] 

*First, build the grid of all possible combinations of education and sex:*
```{r}
education_levels = unique(earnings$education)
sex_levels = unique(earnings$sex)
scenarios = expand_grid(education = education_levels, 
                        sex = sex_levels)
knitr::kable(scenarios)
```
*Then, predict like you've always done:*
```{r}
scenarios['predicted_probability'] = predict(missing_model, 
                                             scenarios, 
                                             type='response') 
knitr::kable(arrange(scenarios, education, sex))
```
4. Interpreting the table from the previous question, which levels of education have the largest difference between men and women in the probability an income is recorded, and which levels have the smallest? Why do you think this pattern occurs? 

*Now, this is an interesting question... it results in a bit of a n-shaped curve, with the lowest probabilities for primary and phd-educated groups, and the highest in the middle:*
```{r}
scenarios %>% group_by(education) %>% 
  summarize(diff = diff(predicted_probability)) %>%
  knitr::kable()
```
For a plot of this, you could try a simple barplot:
```{r}
to_plot = scenarios %>% group_by(education) %>% 
          mutate(education = factor(education, 
                                    levels=c("primary", "secondary", 
                                             "university", "postgraduate", 
                                             "phd")))

ggplot(to_plot %>% summarize(diff = diff(predicted_probability)), 
       aes(x=education, y=diff)) + geom_bar(stat='identity') + 
  labs(x='Education', y="'Extra' probability a man\nhas an income recorded")

```

*Or, something much more complicated, like a dumbbell plot, which requires data in both wide and long form simultaneously!*
```{r, message=F}
library(ggalt) # dumbell plots!

  ggplot(to_plot, aes(y=education)) + 
    geom_point(aes(x=predicted_probability, color=sex)) + 
    geom_dumbbell(data=to_plot %>% pivot_wider(names_from=sex, 
                                               values_from=predicted_probability), 
                  aes(x=male, xend=female),
                  colour_x='blue', colour_xend='red', 
                  size_x=2, size_xend = 2,
                  dot_guide=T) +
    scale_color_manual(name = "", values = c("red", "blue")) +
    labs(x="Predicted probability\nthat a person has income")
```
*This is pretty interesting! As education increases, you see an increasing probability your income is recorded. However, the gap widens, too, until you get to the PhD, where it narrows again.*


```{r}
probabilities = predict(missing_model, type='response')
```

```{r}
library(plotROC)

ggplot(earnings %>%
         mutate(probability = probabilities),
       aes(m = probability, d = has_income)) + 
  geom_roc()
```


```{r}
accuracy = function(threshold, 
                    percent=TRUE, 
                    data=earnings, 
                    model=missing_model){
  n_right = data %>% mutate(predict_prob = predict(model, type='response')) %>%
                     mutate(predict_binary = predict_prob > threshold) %>% 
                     transmute(agreement = predict_binary == has_income) %>%
                     summarize(n_right = sum(agreement)) %>% pull(n_right)
  return(ifelse(percent, 
                n_right/nrow(data), 
                n_right))
}

accuracies <- c()
thresholds = seq(.01, 1, .02)
for(t in thresholds){
  this_accuracy <- accuracy(t)
  accuracies <- c(accuracies, this_accuracy)
}
qplot(thresholds, accuracies, geom='line')
```

[^hint-scatter]: It also may help to build a scatterplot of the earnings vs. height with the regression lines you've just fit. Remember that you can use `geom_smooth(..., method=lm)` to plot the line of best fit. And, you can specify a new `aes()` for each `geom_` element in your `ggplot`. 
[^hint-hasincome]: It will help to build a new column in your dataframe that is zero where income is zero or NA and is 1 elsewhere. You might want to do this first by finding where the income is larger than zero and *then* filling in missing values using `replace_na`. 
[^hint-expandgrid]: Using `tidyr::expand_grid`, you can build a dataframe of all the combinations of two vectors quickly. For example, you could describe most rooms in the university with `tidyr::expand_grid(`
`floor=c("carpet", "tile","linoleum"),`
`walls=c("paint","wood paneling"))`