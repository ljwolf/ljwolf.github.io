<!DOCTYPE html>
<html>
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>throwing in a spatially-correlated random effect may mess up the fixed effect you love - revisiting Hodges and Reich (2010) for SAR models  - Yet Another Geographer</title>
<meta name="description" content="">

<link rel="alternate" type="application/rss+xml" title="RSS" href="/rss/">

<link rel="icon" type="image/x-icon" href="/favicon.ico">
<link rel="apple-touch-icon-precomposed" href="/favicon.png">

<link rel="stylesheet" href="/css/style.css?rnd=1627573598" />

<meta property="og:title" content="throwing in a spatially-correlated random effect may mess up the fixed effect you love - revisiting Hodges and Reich (2010) for SAR models" />
<meta property="og:description" content="import pysal as ps import numpy as np import pandas as pd import matplotlib.pyplot as plt import geopandas as gpd %matplotlib inline This is just a quick demonstration of what I understand from Hodges &amp; Reich (2010)&rsquo;s argument about the structure of spatial error terms. Essentially, his claim is that the substantive estimates ($\hat{\beta}$) from an ordinary least squares regression over $N$ observations and $P$ covariates:
$$ Y \sim \mathcal{N}(X\hat{\beta}, \sigma^2)$$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/hodges-reich-sar/" />
<meta property="article:published_time" content="2018-02-24T15:32:22+00:00" />
<meta property="article:modified_time" content="2018-02-24T15:32:22+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="throwing in a spatially-correlated random effect may mess up the fixed effect you love - revisiting Hodges and Reich (2010) for SAR models"/>
<meta name="twitter:description" content="import pysal as ps import numpy as np import pandas as pd import matplotlib.pyplot as plt import geopandas as gpd %matplotlib inline This is just a quick demonstration of what I understand from Hodges &amp; Reich (2010)&rsquo;s argument about the structure of spatial error terms. Essentially, his claim is that the substantive estimates ($\hat{\beta}$) from an ordinary least squares regression over $N$ observations and $P$ covariates:
$$ Y \sim \mathcal{N}(X\hat{\beta}, \sigma^2)$$"/>






    
</head>
<body>
    <div class="container">
        <header> 
            
                <h1 class="site-header">
    <a href="/">Yet Another Geographer</a>
</h1>
<nav>
    
    
    <a class="" href="/about/" title="">About</a>
    
    <a class="" href="https://www.dropbox.com/s/y5mjkduq7bopzex/currvita.pdf?dl=0" title="">CV</a>
    
    <a class="" href="http://www.bristol.ac.uk/geography/people/levi-j-wolf/overview.html" title="">Affiliation</a>
    
    <a class="" href="/teaching/" title="">Teaching</a>
    
    <a class=" active" href="/posts/" title="Posts">Posts</a>
    
</nav>

            
        </header>
        <main>
            

    <article class="post">
        <header>
            <h1>throwing in a spatially-correlated random effect may mess up the fixed effect you love - revisiting Hodges and Reich (2010) for SAR models</h1>
        </header>
        <div class="content">
            <div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pysal <span style="color:#f92672">as</span> ps
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> geopandas <span style="color:#f92672">as</span> gpd
<span style="color:#f92672">%</span>matplotlib inline
</code></pre></div><p>This is just a quick demonstration of what I understand from <a href="https://www4.stat.ncsu.edu/~bjreich/papers/FixedEffectsYouLove.pdf">Hodges &amp; Reich (2010)</a>&rsquo;s argument about the structure of spatial error terms. Essentially, his claim is that the substantive estimates ($\hat{\beta}$) from an ordinary least squares regression over $N$ observations and $P$ covariates:</p>
<p>$$ Y \sim \mathcal{N}(X\hat{\beta}, \sigma^2)$$</p>
<p>might not be equal to the estimates from a specific type of spatial autoregressive error model, such as the SAR error model:</p>
<p>$$ Y \sim\mathcal{N}\left(X\hat{\beta}, \left[(I - \lambda \mathbf{W})'(I - \lambda \mathbf{W})\right]^{-1}\sigma^2\right) $$</p>
<p>when (and only when) the filtered error term $(I - \lambda \mathbf{W})^{-1}\epsilon$ (for gaussian $\epsilon$ with dispersion $\sigma$) is correlated with at least one $X_j$, $j = 1, 2, \dots, P$.</p>
<p>Hodges &amp; Reich (2010) consider a CAR model, but they argue (and I agree): the logic of the issue applies generally.
The idea also touches on <a href="http://www.tandfonline.com/doi/abs/10.1080/17421770802353758">LeSage &amp; Fischer (2008)</a>&rsquo;s argument motivating a spatial Durbin regression as &ldquo;accounting for spatially-patterned ommitted covariates,&rdquo; (Section 2.1, effective around page 280).</p>
<p>It&rsquo;s pretty simple to show how it might affect regression below, so I&rsquo;ve cooked up a short example using the Baltimore data. I think it&rsquo;s important to understand this (just like apparent variance inflation in simulation designs due to $tr(\left[I - \lambda \mathbf{W}\right]^{-1}) \geq N$).</p>
<p>This is just one of those areas where, in theory, a &ldquo;well-behaved&rdquo; statement leads to results we&rsquo;d want (identical estimates of $\beta$ between OLS and spatial error models), but when we use these models in reality, we must accept that empirical conditions will degrade these expectations.</p>
<p>So I&rsquo;ll illustrate it on a standard PySAL dataset below, trying to show it as simply as possible.</p>
<h3 id="spatial-confounding-in-spatial-error-models">Spatial Confounding in Spatial Error Models</h3>
<p>Let&rsquo;s consider the Baltimore house price dataset:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">data <span style="color:#f92672">=</span> gpd<span style="color:#f92672">.</span>read_file(ps<span style="color:#f92672">.</span>examples<span style="color:#f92672">.</span>get_path(<span style="color:#e6db74">&#39;baltim.shp&#39;</span>))
</code></pre></div><p>Grab a symmetric spatial kernel weight, using a triangular kernel (but you can change it to be whatever you&rsquo;d like), and ensure that the diagonal is zero, since that&rsquo;s necessary for the spatial weights used in the spatial regressions in PySAL.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">weights <span style="color:#f92672">=</span> ps<span style="color:#f92672">.</span>weights<span style="color:#f92672">.</span>Kernel<span style="color:#f92672">.</span>from_dataframe(data, k<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, fixed<span style="color:#f92672">=</span>True, function<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;triangular&#39;</span>)
weights <span style="color:#f92672">=</span> ps<span style="color:#f92672">.</span>weights<span style="color:#f92672">.</span>util<span style="color:#f92672">.</span>fill_diagonal(weights, <span style="color:#ae81ff">0</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">data<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>I&rsquo;ll pick a subset of covariates that tries to avoid clumping/restriction of values, but give a decent enough model fit.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">covariates <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;NROOM&#39;</span>, <span style="color:#e6db74">&#39;AGE&#39;</span>, <span style="color:#e6db74">&#39;LOTSZ&#39;</span>, <span style="color:#e6db74">&#39;SQFT&#39;</span>]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">X <span style="color:#f92672">=</span> data[covariates]<span style="color:#f92672">.</span>values
Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(data[[<span style="color:#e6db74">&#39;PRICE&#39;</span>]]<span style="color:#f92672">.</span>values)
N,P <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>shape
</code></pre></div><p>Now, I&rsquo;m going to fit the ML Error model and the OLS on the same data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mlerr <span style="color:#f92672">=</span> ps<span style="color:#f92672">.</span>spreg<span style="color:#f92672">.</span>ML_Error(Y,X, w<span style="color:#f92672">=</span>weights)
ols <span style="color:#f92672">=</span> ps<span style="color:#f92672">.</span>spreg<span style="color:#f92672">.</span>OLS(Y,X)
</code></pre></div><pre><code>/home/ljw/anaconda3/envs/ana/lib/python3.6/site-packages/scipy/optimize/_minimize.py:643: RuntimeWarning: Method 'bounded' does not support relative tolerance in x; defaulting to absolute tolerance.
  &quot;defaulting to absolute tolerance.&quot;, RuntimeWarning)
/home/ljw/anaconda3/envs/ana/lib/python3.6/site-packages/pysal/spreg/ml_error.py:483: RuntimeWarning: invalid value encountered in log
  jacob = np.log(np.linalg.det(a))
</code></pre>
<p>And, I&rsquo;ll take a look at the two sets of estimates between the two, as well as computing the difference.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">diff <span style="color:#f92672">=</span> (ols<span style="color:#f92672">.</span>betas <span style="color:#f92672">-</span> mlerr<span style="color:#f92672">.</span>betas[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]) 
distinct <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>abs(diff<span style="color:#f92672">.</span>flatten()) <span style="color:#f92672">&gt;</span> (<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>ols<span style="color:#f92672">.</span>std_err)
compare <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame<span style="color:#f92672">.</span>from_dict({<span style="color:#e6db74">&#34;OLS&#34;</span> : ols<span style="color:#f92672">.</span>betas<span style="color:#f92672">.</span>flatten(), 
                        <span style="color:#e6db74">&#34;ML Error&#34;</span> :   mlerr<span style="color:#f92672">.</span>betas[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>flatten(),
                        <span style="color:#e6db74">&#34;Difference&#34;</span> :   diff<span style="color:#f92672">.</span>flatten(),
                        <span style="color:#e6db74">&#34;SE OLS&#34;</span>: ols<span style="color:#f92672">.</span>std_err,
                        <span style="color:#e6db74">&#34;SE ML Error&#34;</span>: mlerr<span style="color:#f92672">.</span>std_err[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>],        
                        <span style="color:#e6db74">&#34;Is Point-Distinct?&#34;</span> : distinct<span style="color:#f92672">.</span>astype(bool)<span style="color:#f92672">.</span>flatten()})
compare<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> ([<span style="color:#e6db74">&#39;Intercept&#39;</span>] <span style="color:#f92672">+</span> covariates)
compare[[<span style="color:#e6db74">&#39;OLS&#39;</span>, <span style="color:#e6db74">&#39;ML Error&#39;</span>, <span style="color:#e6db74">&#39;Difference&#39;</span>, <span style="color:#e6db74">&#39;SE OLS&#39;</span>, <span style="color:#e6db74">&#39;SE ML Error&#39;</span>, <span style="color:#e6db74">&#39;Is Point-Distinct?&#39;</span>]]
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>While the estimates are not interval-distinct (in that all interval estimates overlap), the point estimate from the ML Error regression for the lot size &amp; age covariates <em>do indeed</em> fall outside of the interval estimate of the OLS.</p>
<p>Regardless, the $\beta$ are definitely &ldquo;changed,&rdquo; since their differences are well above the order of numerical precision. While this data is too noisy to show a <em>really</em> dramatic difference, it&rsquo;s dramatic enough for two point estimates to fall outside the 95% confidence interval of another.</p>
<h2 id="whats-collinearity-got-to-do-with-it">What&rsquo;s Collinearity got to do with it?</h2>
<p>OK, so I can show that they&rsquo;re probably changed. And, if Hodges &amp; Reich (2010) are right about this being a collinearity problem, I can show <em>how</em> changed they are by trying to orthogonalize my regressors.</p>
<ol>
<li>So, I&rsquo;m going to draw an $N \times N$ random matrix whose vectors are orthogonal.</li>
<li>Then, I&rsquo;m going to construct a fake Y given the same $\beta$ from the OLS above.</li>
<li>If it&rsquo;s true that the ML Error and OLS return <em>the exact same</em> betas when the error term is not collinear with the covariates, then in this construction, the OLS on the synthetic orthogonal data should recover the same betas as the OLS on the real data. Further, the OLS on the synthetic orthogonal data should <em>also</em> recover the same betas as the ML Error on the synthetic orthogonal data.</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> scipy.stats <span style="color:#f92672">as</span> st
</code></pre></div><p>We can use <code>scipy.stats.ortho_group</code> to draw $N \times N$ matrices of orthogonal vectors. Taking the first $P$ of these will result in a synthetic $X$ matrix with orthogonal covariates. I&rsquo;ll also add some uniform noise between -1,1.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Xortho <span style="color:#f92672">=</span> st<span style="color:#f92672">.</span>ortho_group<span style="color:#f92672">.</span>rvs(weights<span style="color:#f92672">.</span>n)[:,<span style="color:#ae81ff">1</span>:P<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,size<span style="color:#f92672">=</span>(N,P))
</code></pre></div><p>Then, I&rsquo;ll use the same betas from the previous &ldquo;real&rdquo; problem to construct a $Y$:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">inherited_betas <span style="color:#f92672">=</span> ols<span style="color:#f92672">.</span>betas
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Yortho <span style="color:#f92672">=</span> Xortho<span style="color:#f92672">.</span>dot(inherited_betas[<span style="color:#ae81ff">1</span>:]) <span style="color:#f92672">+</span> inherited_betas[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>,ols<span style="color:#f92672">.</span>sig2<span style="color:#f92672">**.</span><span style="color:#ae81ff">5</span>)
</code></pre></div><p>Now, we run the regression and get the $\hat{\beta}$ coefficients:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ols_ortho <span style="color:#f92672">=</span> ps<span style="color:#f92672">.</span>spreg<span style="color:#f92672">.</span>OLS(Yortho, Xortho)
betas_from_ols_orthogonal_data <span style="color:#f92672">=</span> ols_ortho<span style="color:#f92672">.</span>betas
</code></pre></div><p>And we run the error model, too:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mlortho <span style="color:#f92672">=</span> ps<span style="color:#f92672">.</span>spreg<span style="color:#f92672">.</span>ML_Error(Yortho, Xortho, w<span style="color:#f92672">=</span>weights, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ord&#39;</span>)
betas_from_error_orthogonal_data <span style="color:#f92672">=</span> mlortho<span style="color:#f92672">.</span>betas
</code></pre></div><p>Now, are they all equal to within machine precision?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pd<span style="color:#f92672">.</span>DataFrame(np<span style="color:#f92672">.</span>column_stack((betas_from_ols_orthogonal_data,
                 betas_from_error_orthogonal_data[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>],
                 inherited_betas)), columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;Orthogonal OLS&#39;</span>, 
                                             <span style="color:#e6db74">&#39;Orthogonal ML Err&#39;</span>, <span style="color:#e6db74">&#39;Observed OLS&#39;</span>])
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">pd<span style="color:#f92672">.</span>DataFrame(np<span style="color:#f92672">.</span>column_stack((betas_from_ols_orthogonal_data,
                 betas_from_error_orthogonal_data[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], 
                 inherited_betas)), columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;NA&#39;</span>, <span style="color:#e6db74">&#39;Ortho. OLS minus Ortho. ML Error&#39;</span>, 
                                             <span style="color:#e6db74">&#39;Ortho. ML Error minus Observed OLS&#39;</span>])<span style="color:#f92672">.</span>diff(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>dropna(axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Yep, those errors for the substantive effects are down in the $10^{-13}$ range, meaning the estimates are effectively within machine error in Python. The intercept between the orthogonal estimates and the non-orthogonal one looks slightly different, but this might be due to numerical issues in the estimation routines, since we&rsquo;re using perfectly orthogonal covariates.</p>
<p>At the end of the day, what&rsquo;s collinearity got to do with it? Below, I&rsquo;ll plot the regressors in $X$ against the error term in the spatial error model:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> seaborn.apionly <span style="color:#f92672">as</span> sns
sns<span style="color:#f92672">.</span>pairplot(x_vars<span style="color:#f92672">=</span>covariates, y_vars<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;error_model_residual&#39;</span>, 
             data<span style="color:#f92672">=</span>data[covariates]<span style="color:#f92672">.</span>assign(error_model_residual <span style="color:#f92672">=</span> mlerr<span style="color:#f92672">.</span>u),
             kind<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;reg&#39;</span>, plot_kws<span style="color:#f92672">=</span>dict(color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, line_kws<span style="color:#f92672">=</span>dict(color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orangered&#39;</span>, zorder<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)))
</code></pre></div><p><img src="/images/hodges-reich-sar-plot.png" alt="png"></p>
<p>If you recall, the two covariates that had the largest shift from the OLS model to the ML Error model in the empirical dataset were the ones for house age and lot size. These are also marginally correlated with the residual from spatial error model.</p>
<p>However, in the case of the forced-to-be-orthogonal variates, they&rsquo;re forced to be uncorrelated with the residuals.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Hodges &amp; Reich (2010) demonstrate that, as this collinearity gets worse between elements of $X$ and the residual term in a spatial error model, the beta effects will change more and more dramatically.</p>
<p>Further, as I hope this shows, Hodges &amp; Reich (2010), Section 2.5, is true for SAR models. There, they argue that</p>
<blockquote>
<p>&hellip; spatial confounding is not an artifact of the ICAR model, but arises from other, perhaps all, specifications of the intution that measures taken at locations near to each other are more similar than measures taken at distant locations. (p. 330).</p>
</blockquote>
<p>The crux of the issue is the collinearity between the correlated error and the $X$ matrix. If $X$ is collinear with the anticipated structure of $(I - \lambda W)^{-1}$, then $\hat{\beta}$ must change.</p>
<p>This is demonstrably true in the SAR Error example above. When using &ldquo;raw&rdquo; data with spatial multicollinearity, the use of an Error model changes the $\beta$ effects. While it doesn&rsquo;t change them dramatically enough to overcome the standard errors, point estimates are disjoint. When using orthogonal covariates, the estimated effects are exactly the same (within machine precision) between OLS and SAR-Error models.</p>

        </div>
        <div class="article-info">
    
        <div class="article-date">2018-02-24</div>
    
    <div class="article-taxonomies">
        
            
    </div>
</div>
    </article>
    


        </main>
        <footer>
            
                <p>© 2021<br>
Powered by <a target="_blank" href="https://gohugo.io/">Hugo</a>, theme <a target="_blank" href="https://github.com/mitrichius/hugo-theme-anubis">Anubis</a>.
</p>
            
        </footer>
    </div>
</body>
</html>
